<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Support Vector Machine (SVM) | Hongguang </title> <meta name="author" content="Hongguang Wang"> <meta name="description" content="An introduction to support vector machine"> <meta property="og:site_name" content="Hongguang"> <meta property="og:type" content="article"> <meta property="og:title" content="Hongguang | Support Vector Machine (SVM)"> <meta property="og:url" content="https://truman0102.github.io/blog/2023/SVM/"> <meta property="og:description" content="An introduction to support vector machine"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Support Vector Machine (SVM)"> <meta name="twitter:description" content="An introduction to support vector machine"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Hongguang Wang"
        },
        "url": "https://truman0102.github.io/blog/2023/SVM/",
        "@type": "BlogPosting",
        "description": "An introduction to support vector machine",
        "headline": "Support Vector Machine (SVM)",
        
        "sameAs": ["https://github.com/truman0102"],
        
        "name": "Hongguang Wang",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://truman0102.github.io/blog/2023/SVM/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Hongguang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Support Vector Machine (SVM)</h1> <p class="post-meta"> Created in November 11, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/svm"> <i class="fa-solid fa-hashtag fa-sm"></i> SVM</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/data-science"> <i class="fa-solid fa-tag fa-sm"></i> data-science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="what-is-svm">What is SVM?</h2> <p>Support vector machine is a supervised learning algorithm that can be used for both classification and regression. It is a discriminative classifier, which means it tries to find a decision boundary that separates the data points into different classes.</p> <h2 id="how-does-svm-work">How does SVM work?</h2> <p>Let linear discriminant function \(f(x)=w^Tx+b\) be the decision boundary for binary classification, where \(w\) is the weight vector and \(b\) is the bias. The decision boundary is the set of points where \(f(x)=0\).</p> <ul> <li>If \(f(x)&gt;0\), \(y\) is expected to be \(1\).</li> <li>If \(f(x)&lt;0\), \(y\) is expected to be \(-1\).</li> </ul> <p>The biggest difference between SVM and other linear classification models is that it tries to maximise the distance between the nearest points of the two classes, which is called the margin. The points that are closest to the decision boundary are called support vectors. The vector \(w\) is perpendicular to the decision boundary, and the distance between the decision boundary and the support vectors is \(\frac{1}{\|w\|}\). The margin is \(\frac{2}{\|w\|}\).</p> <p>Let \(x_p\) be a point on the decision boundary, which is the projected point of \(x\) outside the boundary, and \(\gamma\) is the distance between \(s\) and \(x_p\). Since \(w\) is perpendicular to the decision boundary,</p> \[\vec{x} = \vec{x_p} + \gamma \frac{\vec{w}}{\lVert \vec{w} \rVert}\] <p>Let’s substitute \(\vec{x}\) with \(\vec{x_p}\) in the linear discriminant function,</p> \[\begin{aligned} f(x) &amp;= w^Tx+b \\ &amp;= w^T(x_p + \gamma \frac{w}{\lVert w \rVert}) + b \\ &amp;= w^Tx_p + \gamma \frac{w^Tw}{\lVert w \rVert} + b \\ &amp;= (w^Tx_p+b) + \gamma \frac{\lVert w \rVert^2}{\lVert w \rVert} \\ &amp;= f(x_p) + \gamma \lVert w \rVert\\ &amp;= 0 + \gamma \lVert w \rVert\\ &amp;= \gamma \lVert w \rVert \end{aligned}\] <p>In other words, the distance between the decision boundary and the support vectors is \(\frac{f(x)}{\lVert w \rVert}\). \(\gamma\) is expected to be positive, so label \(y\) can be used to eliminate the negative sign</p> \[\gamma = \frac{yf(x)}{\lVert w \rVert}\quad y\in\{-1,1\}\] <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ml/SVM_2-480.webp 480w,/assets/img/ml/SVM_2-800.webp 800w,/assets/img/ml/SVM_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ml/SVM_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The minimum distance between the decision boundary and the support vectors is expected to be \(\frac{1}{\lVert w \rVert}\), since the closest point to the decision boundary is expected to satisfy \(yf(x)=1\).</p> <p>The goal of SVM is to maximise the margin, which is equivalent to maximising \(\frac{2}{\lVert w \rVert}\), which is equivalent to minimising \(\lVert w \rVert^2\). The optimisation problem can be written as</p> \[\begin{aligned} &amp;\max_{w,b}\frac{2}{\lVert w \rVert} \\ \text{subject to} \quad &amp; y_i(w^Tx_i+b) \geq 1 \quad \forall i\\ \text{equal to} \quad &amp; \min_{w,b}\frac{1}{2}\lVert w \rVert^2 \\ \end{aligned}\] <p>SVM and Logistic Regression are both linear classification models, but they are different in the optimization, because Logistic Regression is a probabilistic model, while SVM is a geometric model.</p> <h3 id="soft-margin">Soft margin</h3> <p>The hard margin SVM only works when the data is linearly separable, meaning \(y_i(w^Tx_i+b) \geq 1\) for all \(i\). In real life, the data is usually not linearly separable. The soft margin SVM allows some data points to be on the wrong side of the decision boundary by introducing a slack variable \(\xi\).</p> \[y_i(w^Tx_i+b) \geq 1 - \xi_i \quad \forall i\] \[\xi_i = \left\{\begin{matrix} 0 &amp; \text{if } y_i(w^Tx_i+b) \geq 1\\ 1 - y_i(w^Tx_i+b) &amp; \text{otherwise} \end{matrix}\right. \geq 0\] <p>Introducing slack variables is costly, so they must be penalised in the objective function.</p> \[\min_{w,b,C}\frac{1}{2}\lVert w \rVert^2 + C\sum_{i=1}^n\xi_i \\ \text{subject to} \quad y_i(w^Tx_i+b) \geq 1 - \xi_i \quad \forall i\\\] <p>Parameter \(C\) controls the trade-off between the slack variables and the margin. When \(C\) is small, the model will try to maximise the margin, and when \(C\) is large, the model will try to minimise the slack variables. The larger the value of \(C\), the more the model will penalise the slack variables, and the larger \(\lVert w \rVert\) will be, which means the margin (\(\frac{2}{\lVert w \rVert}\)) will be smaller. When \(C\) is close to \(\infty\), the slack variables will be close to \(0\), and the model is equivalent to the hard margin SVM.</p> <p>Here, \(C\sum_{i=1}^n\xi_i\) is actually <code class="language-plaintext highlighter-rouge">Hinge Loss</code> and \(\frac{1}{2}\lVert w \rVert^2\) is <code class="language-plaintext highlighter-rouge">L2 Regularisation</code> (<code class="language-plaintext highlighter-rouge">L1</code> can also be used).</p> \[\xi=L_{Hinge}(y,f(x)) = \left\{\begin{matrix} 0 &amp; \text{if } yf(x) \geq 1\\ 1 - yf(x) &amp; \text{otherwise} \end{matrix}\right.\] <p>When \(yf(x) \geq 1\), the point is on the correct side of the decision boundary, so the loss is \(0\). When \(yf(x) &lt; 1\), the point is on the wrong side of the decision boundary, so the loss is \(1 - yf(x)\), namely \(\xi\).</p> <h2 id="how-to-solve-the-dual-problem">How to solve the dual problem?</h2> <p>The target function of SVM is actually a quadratic programming problem, which can be solved by Lagrange multipliers. The Lagrangian function is</p> \[L(w,b,\alpha) = \frac{1}{2}\lVert w \rVert^2 - \sum_{i=1}^n\alpha_i(y_i(w^Tx_i+b)-1)\; \forall\alpha_i \geq 0\] <p>Let</p> \[\theta(w,b) = \max_{\alpha_i \geq 0}L(w,b,\alpha)\] <p>According to the dual problem, the objective function is</p> \[\min_{w,b}\theta(w,b)=\min_{w,b}\max_{\alpha_i \geq 0}L(w,b,\alpha)=p^*\] <p>Here, \(p^*\) is the optimal value of the primal problem. Changing the order of \(\min\) and \(\max\), the dual problem is</p> \[\max_{\alpha_i \geq 0}\min_{w,b}L(w,b,\alpha)=d^*\] <p>Here, \(d^*\) is the optimal value of the dual problem. The dual problem is always a lower bound of the primal problem, so \(d^* \leq p^*\). If \(d^* = p^*\), the solution is optimal.</p> <p>The first step is to find the saddle point of \(L(w,b,\alpha)\), which is the point where the gradient of \(L(w,b,\alpha)\) is \(0\).</p> \[\begin{aligned} \frac{\partial L}{\partial w} &amp;= w - \sum_{i=1}^n\alpha_iy_ix_i = 0\\ &amp;\Rightarrow w = \sum_{i=1}^n\alpha_iy_ix_i\\ \frac{\partial L}{\partial b} &amp;= -\sum_{i=1}^n\alpha_iy_i = 0\\ &amp;\Rightarrow \sum_{i=1}^n\alpha_iy_i = 0 \end{aligned}\] <p>The second step is to substitute \(w\) and \(b\) with the results from the first step.</p> \[\begin{aligned} L(w,b,\alpha) &amp;= \frac{1}{2}\lVert w \rVert^2 - \sum_{i=1}^n\alpha_i(y_i(w^Tx_i+b)-1)\\ &amp;= \frac{1}{2}w^Tw - \sum_{i=1}^n\alpha_i y_i w^Tx_i - \sum_{i=1}^n\alpha_i y_i b + \sum_{i=1}^n\alpha_i\\ &amp;= \frac{1}{2}\sum_{i=1}^n\alpha_iy_ix_i^T\sum_{j=1}^n\alpha_jy_jx_j - \sum_{i=1}^n\alpha_i y_i \sum_{j=1}^n\alpha_jy_jx_j^Tx_i - \sum_{i=1}^n\alpha_i y_i b + \sum_{i=1}^n\alpha_i\\ &amp;= \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j - b\sum_{i=1}^n\alpha_i y_i + \sum_{i=1}^n\alpha_i\\ &amp;= \sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j \end{aligned}\] <p>Then, the optimazaion problem becomes maximising\(\sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j\) where \(\alpha_i \geq 0\) and \(\sum_{i=1}^n\alpha_iy_i = 0\).</p> <p>After solving the dual problem, the optimal \(\alpha\) can be used to calculate \(w\) and \(b\).</p> \[\begin{aligned} f(x) &amp;= w^Tx+b\\ &amp;= \sum_{i=1}^n\alpha_iy_ix_i^Tx + b\\ &amp;= \sum_{i=1}^n\alpha_iy_i(x_i^Tx) + b\\ &amp;= \sum_{i=1}^n\alpha_iy_i&lt;x_i,x&gt; + b \end{aligned}\] <p>where \(\lessdot\) ,\(\gtrdot\) is the inner product and \(x_i\) is the \(i\)th support vector, size of which is \(d \times 1\), and \(\alpha_i\) is the Lagrange multiplier of \(x_i\).</p> <p>The KKT conditions are</p> \[\begin{aligned} \left\{ \begin{array}{ll} \alpha_i \geq 0 \\ y_i(w^Tx_i+b)-1 \geq 0 \\ \alpha_i(y_i(w^Tx_i+b)-1) = 0 \\ \end{array} \right. \end{aligned}\] <ol> <li>If \(y_i(w^Tx_i+b)-1 = 0\), then \(\alpha_i &gt; 0\).</li> <li>If \(y_i(w^Tx_i+b)-1 &gt; 0\), then \(\alpha_i = 0\).</li> </ol> <h2 id="optimazation-of-soft-margin-svm">Optimazation of Soft Margin SVM</h2> <p>The optimisation target of soft margin SVM is</p> \[\min_{w,b}\frac{1}{2}\lVert w \rVert^2 + C\sum_{i=1}^N\xi_i \\ s.t. \quad y_i(w^Tx_i+b) \geq 1 - \xi_i,\; \xi_i \geq 0,\; C &gt; 0\quad \forall i\] <p>which means \(\lVert w \rVert\) is expected to be small and \(\xi_i\) is expected to be small so that more samples are correctly classified.</p> <p>The Lagrangian function is</p> <p>\(\begin{aligned} L(w,b,\xi,\alpha,\beta) &amp;= \frac{1}{2}\lVert w \rVert^2 + C\sum_{i=1}^N\xi_i\\ &amp;- \sum_{i=1}^N\alpha_i(y_i(w^Tx_i+b)-1+\xi_i) - \sum_{i=1}^N\mu_i\xi_i\\ \end{aligned}\) \(s.t. \quad \alpha_i \geq 0,\; \mu_i \geq 0,\;\xi_i \geq 0\quad \forall i\)</p> <p>The first step is to minimise \(w\), \(b\) and \(\xi\), equivalently let the partial derivatives of \(L(w,b,\xi,\alpha,\beta)\) with respect to \(w\), \(b\) and \(\xi\) be \(0\).</p> \[\begin{aligned} \frac{\partial L}{\partial w} &amp;= w - \sum_{i=1}^N\alpha_iy_ix_i = 0\\ &amp;\Rightarrow w = \sum_{i=1}^N\alpha_iy_ix_i\\ \frac{\partial L}{\partial b} &amp;= -\sum_{i=1}^N\alpha_iy_i = 0\\ &amp;\Rightarrow \sum_{i=1}^N\alpha_iy_i = 0\\ \frac{\partial L}{\partial \xi_i} &amp;= C - \alpha_i - \mu_i = 0\\ &amp;\Rightarrow C=\alpha_i+\mu_i \end{aligned}\] <p>The second is to substitute \(w\), \(b\) and \(\xi\) with the results from the first step.</p> \[\begin{aligned} L(w,b,\xi,\alpha,\beta) &amp;= \frac{1}{2}\lVert w \rVert^2 + C\sum_{i=1}^N\xi_i\\ &amp;- \sum_{i=1}^N\alpha_i(y_i(w^Tx_i+b)-1+\xi_i) - \sum_{i=1}^N\mu_i\xi_i\\ &amp;= \frac{1}{2}w^Tw + C\sum_{i=1}^N\xi_i\\ &amp;- \sum_{i=1}^N\alpha_i(y_i(w^Tx_i+b)-1+\xi_i) - \sum_{i=1}^N\mu_i\xi_i\\ &amp;= \frac{1}{2}(\sum_{i=1}^N\alpha_iy_ix_i)^T(\sum_{i=1}^N\alpha_iy_ix_i) + C\sum_{i=1}^N\xi_i\\ &amp;- \sum_{i=1}^N\alpha_i(y_i(\sum_{i=1}^N\alpha_iy_ix_i^Tx_i)+b)-\sum_{i=1}^N\alpha_i(y_i-1+\xi_i) - \sum_{i=1}^N\mu_i\xi_i\\ &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + C\sum_{i=1}^N\xi_i\\ &amp;- \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N\alpha_iy_ib - \sum_{i=1}^N\alpha_iy_i + \sum_{i=1}^N\alpha_i\xi_i - \sum_{i=1}^N\mu_i\xi_i\\ &amp;= \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j \end{aligned}\] <p>The target function is the same as the one of hard margin SVM, but the dual problem is different because \(C=\alpha_i+\mu_i\Rightarrow 0 \leq \alpha_i \leq C\).</p> <p>The KKT conditions are</p> \[\left\{ \begin{array}{ll} \alpha_i \geq 0 \\ \mu_i \geq 0 \\ \alpha_i(y_i(w^Tx_i+b)-1+\xi_i) = 0 \\ \mu_i\xi_i = 0 \\ y_i(w^Tx_i+b)-1+\xi_i \geq 0 \\ \xi_i \geq 0 \\ \end{array} \right.\] <ol> <li>If \(\alpha_i = 0\), the point is not a support vector.</li> <li>If \(\alpha_i &gt; 0\), the point is a support vector, and \(y_i(w^Tx_i+b) = 1-\xi_i\). <ol> <li>If \(\alpha_i &lt; C\), the point is on the correct side of the decision boundary, and \(\xi_i = 0\).</li> <li>If \(\alpha_i = C\), then \(\mu_i = 0\) <ol> <li>If \(\xi_i \leq 1\), then \(y_i(w^Tx_i+b) \geq 0\), and the point is on the correct side of the decision boundary.</li> <li>If \(\xi_i &gt; 1\), then \(y_i(w^Tx_i+b) &lt; 0\), and the point is on the wrong side of the decision boundary.</li> </ol> </li> </ol> </li> </ol> <h2 id="what-is-the-kernel-trick">What is the kernel trick?</h2> <p>The kernel trick is a way to transform the data into a higher dimension so that it can be linearly separable. Let \(\phi\) be the transformation function, \(d\) be the dimension of original data, and \(D\) be the dimension of transformed data.</p> \[\begin{aligned} f(x) &amp;= w^T\phi(x)+b\\ &amp;= \sum_{i=1}^n\alpha_iy_i\phi(x_i)^T\phi(x)+b\\ &amp;= \sum_{i=1}^n\alpha_iy_iK(x_i,x)+b \end{aligned}\] <p>The kernel function is the inner product of the transformed data:</p> \[k(x,z) = \left\langle \phi(x),\phi(z) \right\rangle\] <p>And the kernel matrix is</p> \[K = \left[\begin{matrix} \left\langle \phi(x_1),\phi(x_1) \right\rangle &amp; \left\langle \phi(x_1),\phi(x_2) \right\rangle &amp; \cdots &amp; \left\langle \phi(x_1),\phi(x_n) \right\rangle\\ \left\langle \phi(x_2),\phi(x_1) \right\rangle &amp; \left\langle \phi(x_2),\phi(x_2) \right\rangle &amp; \cdots &amp; \left\langle \phi(x_2),\phi(x_n) \right\rangle\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \left\langle \phi(x_n),\phi(x_1) \right\rangle &amp; \left\langle \phi(x_n),\phi(x_2) \right\rangle &amp; \cdots &amp; \left\langle \phi(x_n),\phi(x_n) \right\rangle\\ \end{matrix}\right]\] <p>Obviously, the kernel matrix is symmetric.</p> <p>Common kernel functions:</p> <ul> <li>Linear kernel:</li> </ul> \[k(x,z) = x^Tz\] <ul> <li>Polynomial kernel:</li> </ul> \[k(x,z) = (\gamma x^Tz + r)^M\] <ul> <li>Gaussian kernel:</li> </ul> \[k(x,z) = \exp(-\frac{\lVert x-z \rVert^2}{2\sigma^2}) = \exp(-\gamma \lVert x-z \rVert^2)\] <p>where \(\sigma\) is the kernel width, and \(\gamma\) is the inverse of the kernel width. The larger \(\gamma\) is, the narrower the kernel width is, the more complex the model is, and the more likely it is to overfit.</p> <h2 id="support-vector-regression-svr">Support Vector Regression (SVR)</h2> <p>Let \(\epsilon\) be the error tolerance, \(\epsilon\)-insensitive loss function is defined as</p> \[L_{\epsilon}(y,f(x)) = \left\{\begin{matrix} 0 &amp; \text{if } |y-f(x)| \leq \epsilon\\ |y-f(x)|-\epsilon &amp; \text{otherwise} \end{matrix}\right.\] <p>The objective function of SVR is</p> \[\begin{aligned} &amp;\min_{w,b}\frac{1}{2}\lVert w \rVert^2 \\ \text{subject to} \quad &amp; |y_i-f(x_i)| \leq \epsilon \quad \forall i\\ \text{equal to} \quad &amp; -\epsilon \leq y_i-f(x_i) \leq \epsilon \quad \forall i\\ \end{aligned}\] <p>Slack variables (positive) are introduced to allow some data points to be outside the \(\epsilon\)-insensitive tube.</p> \[-\epsilon - \xi_i^{\vee} \leq y_i-f(x_i) \leq \epsilon + \xi_i^{\wedge} \quad \forall i\] </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hongguang Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 11, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-reinforcement-learning-within-tree-search-for-fast-macro-placement",title:"Reinforcement Learning within Tree Search for Fast Macro Placement",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficient-place/"}},{id:"post-reinforcement-learning-policy-as-macro-regulator-rather-than-macro-placer",title:"Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/macro-regulator/"}},{id:"post-transferable-chip-placement-via-offline-decision-transformer",title:"Transferable Chip Placement via Offline Decision Transformer",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chipformer/"}},{id:"post-chip-placement-with-diffusion",title:"Chip Placement with Diffusion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/diffusion-placement/"}},{id:"post-macro-placement-by-wire-mask-guided-black-box-optimization",title:"Macro Placement by Wire-Mask-Guided Black-Box Optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/wiremask-bbo/"}},{id:"post-learning-global-routing-via-hub-generation-and-pin-hub-connection",title:"Learning Global Routing via Hub Generation and Pin-hub Connection",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hubrouter/"}},{id:"post-the-policy-gradient-placement-and-generative-routing-neural-networks-for-chip-design",title:"The Policy-gradient Placement and Generative Routing Neural Networks for Chip Design",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/generative-routing-chip/"}},{id:"post-on-joint-learning-for-solving-placement-and-routing-in-chip-design",title:"On Joint Learning for Solving Placement and Routing in Chip Design",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/joint-learning-chip/"}},{id:"post-macro-placement-for-chip-design",title:"Macro Placement for Chip Design",description:"brief introduction to macro placement in chip design",section:"Posts",handler:()=>{window.location.href="/blog/2024/macro-placement/"}},{id:"post-diffusion-model",title:"Diffusion Model",description:"An introduction to the diffusion model.",section:"Posts",handler:()=>{window.location.href="/blog/2024/diffusion-model/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-swarm-intelligence",title:"Swarm Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/swarm-intelligence/"}},{id:"post-regularization",title:"Regularization",description:"An introduction to regularization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/regularization/"}},{id:"post-causal-effect",title:"Causal Effect",description:"An introduction to causal effect in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2024/causal-effect/"}},{id:"post-advanced-ai-notes",title:"Advanced AI Notes",description:"notes on advanced AI course",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-ai-note/"}},{id:"post-decision-tree",title:"Decision Tree",description:"An introduction to decision trees in Machine Learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/decision-tree/"}},{id:"post-principal-component-analysis-pca",title:"Principal Component Analysis (PCA)",description:"An introduction to PCA, one of the most popular dimensionality reduction techniques in data science.",section:"Posts",handler:()=>{window.location.href="/blog/2023/PCA/"}},{id:"post-ensemble-learning",title:"Ensemble Learning",description:"An introduction to ensemble learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ensemble-learning/"}},{id:"post-linear-regression",title:"Linear Regression",description:"An introduction to linear regression.",section:"Posts",handler:()=>{window.location.href="/blog/2023/linear-regression/"}},{id:"post-generative-classifier",title:"Generative Classifier",description:"An introduction to generative classifiers and their use in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/gen-classifier/"}},{id:"post-linear-discriminant-classifier",title:"Linear Discriminant Classifier",description:"An introduction to the Linear Discriminant Classifier in Machine Learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/linear-discriminant-classifier/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-adversarial-search",title:"Adversarial Search",description:"An introduction to adversarial search in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/adversarial-search/"}},{id:"post-sovling-problems-by-searching",title:"Sovling Problems by Searching",description:"An introduction to search in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/search/"}},{id:"post-first-order-logic",title:"First-Order Logic",description:"An introduction to first-order logic in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/first-order-logic/"}},{id:"post-logic-agents",title:"Logic Agents",description:"An introduction to logic agents in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/logic-agents/"}},{id:"post-parameter-estimation",title:"Parameter Estimation",description:"An introduction to parameter estimation in data science.",section:"Posts",handler:()=>{window.location.href="/blog/2023/parameter-estimation/"}},{id:"post-support-vector-machine-svm",title:"Support Vector Machine (SVM)",description:"An introduction to support vector machine",section:"Posts",handler:()=>{window.location.href="/blog/2023/SVM/"}},{id:"post-logistic-regression",title:"Logistic Regression",description:"An introduction to logistic regression",section:"Posts",handler:()=>{window.location.href="/blog/2023/logistic-regression/"}},{id:"post-normalization-form",title:"Normalization Form",description:"An introduction to normalization form in database systems.",section:"Posts",handler:()=>{window.location.href="/blog/2023/norm-form/"}},{id:"post-functional-dependencies",title:"Functional Dependencies",description:"An introduction to functional dependencies in database systems.",section:"Posts",handler:()=>{window.location.href="/blog/2023/functional-dependencies/"}},{id:"post-relational-algebra",title:"Relational Algebra",description:"An introduction to relational algebra in database systems.",section:"Posts",handler:()=>{window.location.href="/blog/2023/relational-algebra/"}},{id:"post-convolution",title:"Convolution",description:"An introduction to convolution in image processing.",section:"Posts",handler:()=>{window.location.href="/blog/2023/convolution/"}},{id:"post-covariance",title:"Covariance",description:"An introduction to covariance.",section:"Posts",handler:()=>{window.location.href="/blog/2023/covariance/"}},{id:"post-probability-distribution",title:"Probability Distribution",description:"An introduction to probability distribution in Probability and Statistics.",section:"Posts",handler:()=>{window.location.href="/blog/2023/probability-distribution/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-eigenvalue-and-eigenvector",title:"Eigenvalue and eigenvector",description:"An introduction to eigenvalue and eigenvector in linear algebra.",section:"Posts",handler:()=>{window.location.href="/blog/2023/eigenvector/"}},{id:"post-vector",title:"Vector",description:"An introduction to vector space and vector operations.",section:"Posts",handler:()=>{window.location.href="/blog/2023/vector/"}},{id:"post-basic-linux-commands",title:"Basic Linux Commands",description:"An introduction to basic Linux commands",section:"Posts",handler:()=>{window.location.href="/blog/2023/linux-command/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-getting-back-to-recording-and-organizing-my-study-notes-including-but-not-limited-to-ai-algorithms-computer-composition-and-operating-systems-data-structures-and-more",title:"Getting back to recording and organizing my study notes, including but not limited...",description:"",section:"News"},{id:"projects-work-item-template",title:"Work Item Template",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-fun-item-template",title:"Fun Item Template",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%72%75%6D%61%6E%70%6B%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/truman0102","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>