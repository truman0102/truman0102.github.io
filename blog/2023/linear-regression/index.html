<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Regression | Hongguang </title> <meta name="author" content="Hongguang Wang"> <meta name="description" content="An introduction to linear regression."> <meta property="og:site_name" content="Hongguang"> <meta property="og:type" content="article"> <meta property="og:title" content="Hongguang | Linear Regression"> <meta property="og:url" content="https://truman0102.github.io/blog/2023/linear-regression/"> <meta property="og:description" content="An introduction to linear regression."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Linear Regression"> <meta name="twitter:description" content="An introduction to linear regression."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Hongguang Wang"
        },
        "url": "https://truman0102.github.io/blog/2023/linear-regression/",
        "@type": "BlogPosting",
        "description": "An introduction to linear regression.",
        "headline": "Linear Regression",
        
        "sameAs": ["https://github.com/truman0102"],
        
        "name": "Hongguang Wang",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://truman0102.github.io/blog/2023/linear-regression/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Hongguang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Regression</h1> <p class="post-meta"> Created in December 18, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/data-science"> <i class="fa-solid fa-tag fa-sm"></i> data-science</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h2"><a href="#cost-function">Cost Function</a></li> <li class="toc-entry toc-h2"><a href="#regularization">Regularization</a></li> <li class="toc-entry toc-h2"> <a href="#optimization">Optimization</a> <ul> <li class="toc-entry toc-h3"><a href="#analytical-solution">Analytical Solution</a></li> <li class="toc-entry toc-h3"><a href="#gradient-descent">Gradient Descent</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>Linear regression is a supervised learning algorithm that is used to predict a continuous variable. It is one of the simplest machine learning algorithms and is often used as a baseline for other algorithms. It is also used to determine the relationship between two variables and how one variable affects the other. For example, it can be used to determine how the price of a house is affected by its size, number of bedrooms, etc.</p> <p>Given a set of training data \(D=\{x_i,y_i\}_{i=1}^N\), where \(x_i\) is the input and \(y_i\) is the target, the goal of linear regression is to find a function \(f(x)\) that minimizes the error between the predicted value \(f(x_i)\) and the actual value \(y_i\). The function \(f(x)\) is called the hypothesis function and is defined as:</p> \[f(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\] <p>where \(w_0\) is the bias term and \(w_1, w_2, \dots, w_n\) are the weights. The bias term is a constant that is added to the output of the linear regression model. The weights are the coefficients of the input variables. The hypothesis function can also be written in vector form as:</p> \[\hat{y} = f(x) = w^Tx\] <p>where \(w\) is the weight vector and \(x\) is the input vector. The weight vector is defined as:</p> \[w = \begin{bmatrix}w_0 \\ w_1 \\ w_2 \\ \vdots \\ w_n\end{bmatrix}\] <p>and the input vector is defined as:</p> \[x = \begin{bmatrix}1 \\ x_1 \\ x_2 \\ \vdots \\ x_n\end{bmatrix}\] <h2 id="cost-function">Cost Function</h2> <p>The cost function is the residual sum of squares (RSS), used to measure the error between the predicted value \(f(x_i)\) and the actual value \(y_i\):</p> \[\text{RSS} = \sum_{i=1}^N (y_i - f(x_i))^2 = \sum_{i=1}^N r_i^2\] <p>where \(r_i\) is the residual. Minimizing the RSS is equivalent to the MLE of white Gaussian noise. The proof is shown below:</p> <p>Let \(\epsilon_i = y_i - f(x_i)\) be the error term. The probability density function of white Gaussian noise is:</p> \[p(\epsilon_i) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\epsilon_i^2}{2\sigma^2}}\] <p>In other words,</p> \[p(y_i|x_i) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - f(x_i))^2}{2\sigma^2}}\] <p>where \(f(x_i)\) is a constant, \(y_i\) is a random variable, whose distribution is given by the above equation. The likelihood function is:</p> \[L = \prod_{i=1}^N p(y_i|x_i) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - f(x_i))^2}{2\sigma^2}}\] <p>Taking the log of the likelihood function:</p> \[\ln L = \sum_{i=1}^N \ln \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - f(x_i))^2}{2\sigma^2}} = -\frac{N}{2}\ln 2\pi\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - f(x_i))^2\] <p>To maximize the likelihood function, we need to minimize the negative log likelihood function, equivalent to minimizing \(\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - f(x_i))^2\), which is a constant times the RSS.</p> <p>The distribution of the residuals is also used to determine whether the linear regression model is a good fit for the data. If the residuals are normally distributed with a mean of 0, then the linear regression model is a good fit for the data.</p> <p>Other cost functions can also be used, such as the mean absolute error (MAE, L1 loss):</p> \[\text{MAE} = \sum_{i=1}^N |y_i - f(x_i)|\] <p>and the Huber loss:</p> \[\text{Huber} = \sum_{i=1}^N \begin{cases} \frac{1}{2}(y_i - f(x_i))^2 &amp; \text{if } |y_i - f(x_i)| \leq \delta \\ \delta|y_i - f(x_i)| - \frac{1}{2}\delta^2 &amp; \text{otherwise} \end{cases}\] <p>where \(\delta\) is a hyperparameter that determines the threshold for the Huber loss. The Huber loss is a combination of the MAE and the RSS. When the residual is small, the Huber loss is equivalent to the RSS. When the residual is large, the Huber loss is equivalent to the MAE.</p> <p>The residual of MAE obeys the laplace distribution,</p> \[p(\epsilon_i) = \frac{1}{2b}e^{-\frac{|\epsilon_i|}{b}}\] <p>where \(b\) is the scale parameter. And the likelihood function is:</p> \[L = \prod_{i=1}^N p(y_i|x_i) = \prod_{i=1}^N \frac{1}{2b}e^{-\frac{|y_i - f(x_i)|}{b}}\] <p>Taking the log of the likelihood function:</p> \[\ln L = \sum_{i=1}^N \ln \frac{1}{2b}e^{-\frac{|y_i - f(x_i)|}{b}} = -N\ln 2b - \frac{1}{b}\sum_{i=1}^N |y_i - f(x_i)|\] <p>To maximize the likelihood function, we need to minimize the negative log likelihood function, equivalent to minimizing \(\frac{1}{b}\sum_{i=1}^N \vert y_i - f(x_i)\vert\), which is a constant times the MAE, similar to the RSS.</p> <h2 id="regularization">Regularization</h2> <p>Regularization is used to prevent overfitting. It is a technique that adds a penalty term to the cost function. The penalty term is a function of the weights and is used to penalize large weights. The cost function with regularization is:</p> \[J = RSS + \lambda R(w) = \sum_{i=1}^N (y_i - f(x_i))^2 + \lambda R(w)\] <p>where \(\lambda\) is the regularization parameter and \(R(w)\) is the regularization term. The regularization term can be L1 regularization:</p> \[R(w) = \sum_{i=1}^N |w_i|\] <p>or L2 regularization:</p> \[R(w) = \sum_{i=1}^N w_i^2\] <p>L1 regularization is also known as Lasso regression and L2 regularization is also known as Ridge regression. It the training data and the weights are represented as matrices, \(X_{N \times D}\), \(Y_{N \times 1}\), and \(W_{D \times 1}\), then the cost function of L1 regularization is:</p> \[J = RSS + \lambda R(w) = (Y - XW)^T(Y - XW) + \lambda |W|\] <p>and the cost function of L2 regularization is:</p> \[\begin{aligned} J &amp;= RSS + \lambda R(w) \\ &amp;= (Y - XW)^T(Y - XW) + \lambda W^TW \\ &amp;= (Y^T - W^TX^T)(Y - XW) + \lambda W^TW \\ \end{aligned}\] <p>The bayesian interpretation of L2 regularization is that it is equivalent to a Gaussian prior on the weights. The proof is shown below:</p> <p>The likelihood function is:</p> \[L = \prod_{i=1}^N p(y_i|x_i) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - f(x_i))^2}{2\sigma^2}}\] <p>Taking the log of the likelihood function:</p> \[\ln L = \sum_{i=1}^N \ln \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - f(x_i))^2}{2\sigma^2}} = -\frac{N}{2}\ln 2\pi\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - f(x_i))^2\] <p>The prior distribution of the weights is:</p> \[\begin{aligned} p(w_i) &amp;= \frac{1}{\sqrt{2\pi\tau^2}}e^{-\frac{w_i^2}{2\tau^2}} \\ p(W) &amp;= \prod_{i=1}^N \frac{1}{\sqrt{2\pi\tau^2}}e^{-\frac{w_i^2}{2\tau^2}} \\ \ln p(W) &amp;= \sum_{i=1}^N \ln \frac{1}{\sqrt{2\pi\tau^2}}e^{-\frac{w_i^2}{2\tau^2}} = -\frac{N}{2}\ln 2\pi\tau^2 - \frac{1}{2\tau^2}\sum_{i=1}^N w_i^2 \\ \end{aligned}\] <p>The posterior distribution of the weights \(\ln p(W\vert X,Y)\propto \ln p(Y\vert X,W) + \ln p(W)\):</p> \[\begin{aligned} \ln p(W|X,Y) &amp;\propto \ln p(Y|X,W) + \ln p(W) \\ &amp;= \ln L + \ln p(W) \\ &amp;= -\frac{N}{2}\ln 2\pi\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - f(x_i))^2 -\frac{N}{2}\ln 2\pi\tau^2 - \frac{1}{2\tau^2}\sum_{i=1}^N w_i^2 \\ \end{aligned}\] <p>To maximize the posterior distribution, we need to minimize the negative log posterior distribution, equivalent to minimizing \(\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - f(x_i))^2 + \frac{1}{2\tau^2}\sum_{i=1}^N w_i^2\), which is a constant times the RSS plus the L2 regularization term. The proof of L1 regularization and laplace prior is similar, and is omitted here.</p> <h2 id="optimization">Optimization</h2> <h3 id="analytical-solution">Analytical Solution</h3> <p>Omitting regularization, the cost function, named the RSS, is:</p> \[J = (Y - XW)^T(Y - XW)\] <p>To minimize the cost function, we take the derivative of the cost function with respect to the weights \(W\) and set it to 0:</p> \[\begin{aligned} \frac{\partial J}{\partial W} &amp;= \frac{\partial}{\partial W} (Y - XW)^T(Y - XW) \\ &amp;= \frac{\partial}{\partial W} (Y^T - W^TX^T)(Y - XW) \\ &amp;= \frac{\partial}{\partial W} (Y^TY - Y^TXW - W^TX^TY + W^TX^TXW) \\ &amp;= -2X^TY + 2X^TXW = 0 \\ X^TXW &amp;= X^TY \\ W &amp;= (X^TX)^{-1}X^TY \\ \end{aligned}\] <p>To get the inverse of \(X^TX\), SVD is used:</p> \[X = U\Sigma V^T\] <p>where \(U\) is an \(N \times N\) orthogonal matrix, \(\Sigma\) is an \(N \times D\) diagonal matrix, and \(V\) is a \(D \times D\) orthogonal matrix. The inverse of \(X^TX\) is:</p> \[\begin{aligned} X^TX &amp;= (U\Sigma V^T)^T(U\Sigma V^T) \\ &amp;= V\Sigma^T U^TU\Sigma V^T \\ &amp;= V\Sigma^T\Sigma V^T \\ &amp;= V\Sigma^2 V^T \\ V^{-1} &amp;= V^T \\ (X^TX)^{-1} &amp;= (V\Sigma^2 V^T)^{-1} \\ &amp;= V\Sigma^{-2} V^T \\ \end{aligned}\] <p>The entries of \(\Sigma^{-2}\) are the reciprocals of the diagonal entries of \(\Sigma^2\), and are the reciprocals of the eigenvalues of \(X^TX\).</p> <h3 id="gradient-descent">Gradient Descent</h3> <p>Gradient descent is an iterative optimization algorithm that is used to find the minimum of a function. It is used to find the weights that minimize the cost function. The weights are initialized and are updated iteratively:</p> \[W_{t+1} = W_t - \alpha \frac{\partial J}{\partial W}\] <p>where \(\alpha\) is the learning rate. The learning rate determines the step size of the gradient descent algorithm. If the learning rate is too small, the algorithm will take a long time to converge. If the learning rate is too large, the algorithm will not converge. The gradient descent algorithm is:</p> <ol> <li>Initialize the weights \(W\).</li> <li>Calculate the gradient \(\frac{\partial J}{\partial W}\).</li> <li>Update the weights \(W\).</li> <li>Repeat steps 2 and 3 until convergence.</li> <li>Return the weights \(W\).</li> </ol> <p>From the <a href="#analytical-solution">analytical solution</a>, the gradient of the cost function is:</p> \[\frac{\partial J}{\partial W} = -2X^TY + 2X^TXW\] <p>If L2 regularization is used, the gradient of the cost function is:</p> \[\frac{\partial J}{\partial W} = -2X^TY + 2X^TXW + 2\lambda W\] </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hongguang Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 04, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-diffusion-model",title:"Diffusion Model",description:"An introduction to the diffusion model.",section:"Posts",handler:()=>{window.location.href="/blog/2024/diffusion-model/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-swarm-intelligence",title:"Swarm Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/swarm-intelligence/"}},{id:"post-regularization",title:"Regularization",description:"An introduction to regularization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/regularization/"}},{id:"post-causal-effect",title:"Causal Effect",description:"An introduction to causal effect in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2024/causal-effect/"}},{id:"post-advanced-ai-notes",title:"Advanced AI Notes",description:"notes on advanced AI course",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-ai-note/"}},{id:"post-decision-tree",title:"Decision Tree",description:"An introduction to decision trees in Machine Learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/decision-tree/"}},{id:"post-principal-component-analysis-pca",title:"Principal Component Analysis (PCA)",description:"An introduction to PCA, one of the most popular dimensionality reduction techniques in data science.",section:"Posts",handler:()=>{window.location.href="/blog/2023/PCA/"}},{id:"post-ensemble-learning",title:"Ensemble Learning",description:"An introduction to ensemble learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ensemble-learning/"}},{id:"post-linear-regression",title:"Linear Regression",description:"An introduction to linear regression.",section:"Posts",handler:()=>{window.location.href="/blog/2023/linear-regression/"}},{id:"post-generative-classifier",title:"Generative Classifier",description:"An introduction to generative classifiers and their use in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/gen-classifier/"}},{id:"post-linear-discriminant-classifier",title:"Linear Discriminant Classifier",description:"An introduction to the Linear Discriminant Classifier in Machine Learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/linear-discriminant-classifier/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-adversarial-search",title:"Adversarial Search",description:"An introduction to adversarial search in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/adversarial-search/"}},{id:"post-sovling-problems-by-searching",title:"Sovling Problems by Searching",description:"An introduction to search in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/search/"}},{id:"post-first-order-logic",title:"First-Order Logic",description:"An introduction to first-order logic in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/first-order-logic/"}},{id:"post-logic-agents",title:"Logic Agents",description:"An introduction to logic agents in artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2023/logic-agents/"}},{id:"post-parameter-estimation",title:"Parameter Estimation",description:"An introduction to parameter estimation in data science.",section:"Posts",handler:()=>{window.location.href="/blog/2023/parameter-estimation/"}},{id:"post-support-vector-machine-svm",title:"Support Vector Machine (SVM)",description:"An introduction to support vector machine",section:"Posts",handler:()=>{window.location.href="/blog/2023/SVM/"}},{id:"post-logistic-regression",title:"Logistic Regression",description:"An introduction to logistic regression",section:"Posts",handler:()=>{window.location.href="/blog/2023/logistic-regression/"}},{id:"post-normalization-form",title:"Normalization Form",description:"An introduction to normalization form in database systems.",section:"Posts",handler:()=>{window.location.href="/blog/2023/norm-form/"}},{id:"post-functional-dependencies",title:"Functional Dependencies",description:"An introduction to functional dependencies in database systems.",section:"Posts",handler:()=>{window.location.href="/blog/2023/functional-dependencies/"}},{id:"post-relational-algebra",title:"Relational Algebra",description:"An introduction to relational algebra in database systems.",section:"Posts",handler:()=>{window.location.href="/blog/2023/relational-algebra/"}},{id:"post-convolution",title:"Convolution",description:"An introduction to convolution in image processing.",section:"Posts",handler:()=>{window.location.href="/blog/2023/convolution/"}},{id:"post-covariance",title:"Covariance",description:"An introduction to covariance.",section:"Posts",handler:()=>{window.location.href="/blog/2023/covariance/"}},{id:"post-probability-distribution",title:"Probability Distribution",description:"An introduction to probability distribution in Probability and Statistics.",section:"Posts",handler:()=>{window.location.href="/blog/2023/probability-distribution/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-eigenvalue-and-eigenvector",title:"Eigenvalue and eigenvector",description:"An introduction to eigenvalue and eigenvector in linear algebra.",section:"Posts",handler:()=>{window.location.href="/blog/2023/eigenvector/"}},{id:"post-vector",title:"Vector",description:"An introduction to vector space and vector operations.",section:"Posts",handler:()=>{window.location.href="/blog/2023/vector/"}},{id:"post-basic-linux-commands",title:"Basic Linux Commands",description:"An introduction to basic Linux commands",section:"Posts",handler:()=>{window.location.href="/blog/2023/linux-command/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-getting-back-to-recording-and-organizing-my-study-notes-including-but-not-limited-to-ai-algorithms-computer-composition-and-operating-systems-data-structures-and-more",title:"Getting back to recording and organizing my study notes, including but not limited...",description:"",section:"News"},{id:"projects-work-item-template",title:"Work Item Template",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-fun-item-template",title:"Fun Item Template",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%72%75%6D%61%6E%70%6B%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/truman0102","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>