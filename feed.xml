<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://truman0102.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://truman0102.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-11T05:13:16+00:00</updated><id>https://truman0102.github.io/feed.xml</id><title type="html">Hongguang</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Transferable Chip Placement via Offline Decision Transformer</title><link href="https://truman0102.github.io/blog/2024/chipformer/" rel="alternate" type="text/html" title="Transferable Chip Placement via Offline Decision Transformer"/><published>2024-12-09T16:00:00+00:00</published><updated>2024-12-09T16:00:00+00:00</updated><id>https://truman0102.github.io/blog/2024/chipformer</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/chipformer/"><![CDATA[<h2 id="introduction">Introduction</h2> <ul> <li>offline RL</li> <li>transferable placement policy</li> <li>offline data are collected from multiple chip circuits using (near) expert-level placement behaviors, differing from the common offline RL setting where the data are collected by sub-optimal behavior policies over a single environment.</li> <li>transformer placement network</li> </ul> <h2 id="methodology">Methodology</h2> <p>离线强化学习的第一个要素就是离线数据的收集，这篇工作提供了12个芯片电路任务的\(500\times 12\)<a href="https://drive.google.com/drive/folders/1F7075SvjccYk97i2UWhahN_9krBvDCmr">专家级布局结果</a>，数据量比较大，很有研究价值。</p> <h3 id="offline-rl">Offline RL</h3> <p>给定一个轨迹\(\tau=(s_1,a_1,\dots,s_T,a_T)\)，其中\(s_t\)是状态，\(a_t\)是动作，可以定义一个后视信息\(\text{HI}(\tau)\)，可以理解成一个隐式可学习特征，用于指导决策，可以是任何函数，自由度较高，在一些工作中可以是累计回报，也可以是最终状态，本文是电路的可学习的embedding。本文的训练目标是最大化条件策略</p> \[\max_{\theta}\mathbb{E}_{(c,\tau)\sim D}\left[\sum_{t=1}^{T}\log\pi_{\theta}(a_t|\tau_t,\text{HI}(c,\tau))\right]\] <p>其中\(D\)是经验轨迹数据集，\(c\)是电路，\(\tau_t\)是直到时间\(t\)的轨迹。</p> <p>电路\(c\)可以表示为无向图\(g^c\)，邻接矩阵\(A^c\in\{0,1\}^{N\times N}\)，其中\(N\)是节点数，节点特征矩阵\(X^c\in\mathbb{R}^{N\times D}\)，\(D\)是节点特征维度。参考VAE的思想，优化变分下界</p> \[\max_{\phi}\mathbb{E}_{c\sim D}\left[\mathbb{E}_{q_\phi(Z\vert X,A)}\log p(A\vert Z)-\text{KL}(q(Z\vert X,A)\Vert p(Z))\right]\] <p>其中\(q_\phi(Z\vert X,A)=\prod_{i=1}^{N}q_\phi(z_i\vert X,A)\)是来自编码器的后验分布，\(p(A\vert Z)=\prod_{i=1}^{N}\prod_{j=1}^{N}\sigma(z_i^Tz_j)\)是来自解码器的似然分布，\(Z=\left[z_1,\dots,z_N\right]^T\)是隐变量，\(p(Z)\)是高斯先验分布。可以看出，这个模型是一个图自编码器，用于学习电路的embedding，所以用节点隐变量的均值来近似替代电路的embedding\(\text{HI}(c,\tau)=\text{HI}(c)=\frac{1}{N}\sum_{i=1}^{N}z_i\)。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper_reading/chipformer-480.webp 480w,/assets/img/paper_reading/chipformer-800.webp 800w,/assets/img/paper_reading/chipformer-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper_reading/chipformer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>模型结构以GPT为backbone，输入包括state token和action token，以及电路的embedding，其中state token参考了MaskPlace工作，包括了position mask、wire mask和view mask三个通道，action token是二维坐标，这两个token在输入前面都有一个embedding layer进行编码。</p> <p>最后是在未见电路上的微调，优化目标与条件策略是类似的，但考虑到微调时每个轮次生成的轨迹是不同的，这些轨迹可能有好有坏，所以根据轨迹的回报对其策略进行加权，此外增加了一个策略熵的正则，以增加探索性。最终的微调目标是</p> \[\min_\theta -\mathbb{E}_{\mathcal{B(\tau)}}\left[\omega(\tau)\log_\theta(a_t\vert \tau_t,\text{HI}(c))\right] + \lambda\max(0,\beta+\mathbb{E}_{\mathcal{B(\tau)}}[\ \log_\theta(\cdot\vert s_t,\text{HI}(c))]\] <p>其中\(\omega(\tau)=\frac{\exp(R(\tau)/\alpha)}{\mathbb{E}_{\mathcal{B(\tau)}}\exp(R(\tau)/\alpha)}\)是轨迹的权重，\(R(\tau)\)是轨迹的回报，\(\alpha\)是温度参数，\(\beta\)是熵的最小值。</p> <h2 id="experiment">Experiment</h2>]]></content><author><name></name></author><category term="paper-reading"/><category term="chip"/><category term="RL"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Chip Placement with Diffusion</title><link href="https://truman0102.github.io/blog/2024/diffusion-placement/" rel="alternate" type="text/html" title="Chip Placement with Diffusion"/><published>2024-12-09T15:00:00+00:00</published><updated>2024-12-09T15:00:00+00:00</updated><id>https://truman0102.github.io/blog/2024/diffusion-placement</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/diffusion-placement/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>diffusion model for macro placement</p> <h2 id="methodology">Methodology</h2> <p>GNN and Attention</p> <h2 id="experiment">Experiment</h2>]]></content><author><name></name></author><category term="paper-reading"/><category term="chip"/><category term="RL"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Macro Placement by Wire-Mask-Guided Black-Box Optimization</title><link href="https://truman0102.github.io/blog/2024/wiremask-bbo/" rel="alternate" type="text/html" title="Macro Placement by Wire-Mask-Guided Black-Box Optimization"/><published>2024-12-09T14:00:00+00:00</published><updated>2024-12-09T14:00:00+00:00</updated><id>https://truman0102.github.io/blog/2024/wiremask-bbo</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/wiremask-bbo/"><![CDATA[<h2 id="introduction">Introduction</h2> <h2 id="background">Background</h2> <p>这篇文章的背景部分概括问题比较好，因此我直接引用了原文的内容。</p> <h3 id="macro-placement">Macro Placement</h3> <p>Macro placement的输入是网表\(H=(V,E)\)，\(V\)是所有cell的信息，\(E\)是所有的边的信息，输出是所有macro的二维坐标。</p> <p>HPWL是常用的评价指标，也可以作为RL的reward，具体来说就是\(E\)中所有边构成的矩形的半周长之和。</p> <p>拥塞度是另一个重要的指标，RUDY为常见的计算拥塞度的方法。具体计算方法为取前\(10\%\)的grid，构成一个子图\(G^{\prime}\)，然后计算\(G^{\prime}\)中每个点所属的所有超边的矩形区域的平均拥塞度\(\frac{1}{\vert G^{\prime} \vert}\sum_{g_i\in G^{\prime}}\sum_{e_j\in E(g_i)}\frac{w_j+h_j}{w_j\cdot h_j}\)。</p> <p>密度用于衡量重叠程度，由于无重叠是宏布局的硬约束，因此有些方法中不作讨论。</p> <p>面积是包围所有宏的最小矩形的面积，在一个固定面积的芯片中布局时，优化目标一般不是面积而是HPWL等。</p> <h3 id="packing-based-methods">Packing-based Methods</h3> <p>macro placement任务中，每个macro以矩形表示，在芯片内部进行放置。目前几种表示方法有sequence-pair, \(\text{B}^\prime\)树, corner block list等。</p> <p>对于marco和标准单元的放置一般采用分而治之的方法，对标准单元进行聚类，然后对marco和类团进行放置，再对类团重新分配，优化布局。聚类方法虽然能缩减任务规模，但可能会切断一些连接，妨碍找到最优解。</p> <h3 id="analytical-methods">Analytical Methods</h3> <p>Analytical methods是一种基于数学模型的方法，通过建立数学模型，求解最优解，以DREAMPlce为代表，将整个placement任务视为最小化目标函数的问题，目标函数包括平滑加权平均线长和密度。分析方法的缺点是不能保证单元之间不重叠，即不能保证硬约束。</p> <h3 id="intensive-reward-in-maskplace">Intensive Reward in MaskPlace</h3> <p>将放置macro后HPWL的增加值作为reward，提供即时奖励。</p> <h2 id="methodology">Methodology</h2> <ul> <li>对macro进行排序，依据是与其相连接的单元的面积和。</li> <li>wire-mask-guided placement: 通过计算满足硬约束位置的HPWL增量来指导macro的有序放置。</li> <li>black-box optimization <ul> <li>random search</li> <li>bayesian optimization</li> <li>evolutionary algorithm: 随机选择两个位置交换位置</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-reading"/><category term="chip"/><category term="RL"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Learning Global Routing via Hub Generation and Pin-hub Connection</title><link href="https://truman0102.github.io/blog/2024/hubrouter/" rel="alternate" type="text/html" title="Learning Global Routing via Hub Generation and Pin-hub Connection"/><published>2024-12-04T19:00:00+00:00</published><updated>2024-12-04T19:00:00+00:00</updated><id>https://truman0102.github.io/blog/2024/hubrouter</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/hubrouter/"><![CDATA[<h2 id="introduciton">Introduciton</h2> <h2 id="fundamentals">Fundamentals</h2> <h3 id="rsmt">RSMT</h3> <p>Rectilinear Steiner 最小树问题（Rectilinear Steiner Minimum Tree，简称 RSMT）是一个组合优化问题，旨在通过添加额外的 Steiner 点，在二维平面上使用仅水平和垂直线段（曼哈顿距离）连接给定的一组点（称为终端点），使得连接的总线长最短。相较于最小生成树（Minimum Spanning Tree，MST）只连接终端点，RSMT 允许添加 Steiner 点，从而可能构造出总长度更短的连通网络。</p> <p>RSMT 问题在集成电路设计、网络布线等领域具有重要应用。然而，由于该问题被证明是 NP 完全问题，所以精确求解在计算上是不可行的，通常需要使用启发式算法或近似算法来获得近似解，用R-MST来近似，其时间复杂度为\(On\log n\)，长度最多为最优RSMT的\(1.5\)倍。</p> <h3 id="hub">Hub</h3> <p>Hub是网格中的一个连接点，类似于交通路段的路口，根据上下左右网格的连接情况，可以分为四种类型：</p> \[\begin{aligned} &amp;+: &amp;r_{(i-1)j} = r_{(i+1)j} = r_{i(j-1)} = r_{i(j+1)} = 1 \\ &amp;\perp: &amp;r_{(i-1)j} + r_{(i+1)j} = r_{i(j-1)} + r_{i(j+1)} = 3 \\ &amp;\llcorner: &amp;r_{(i-1)j} + r_{(i+1)j} = 1 \;\text{and}\; r_{i(j-1)} + r_{i(j+1)} = 1 \\ &amp;\cdot: &amp;r_{(i-1)j} + r_{(i+1)j} + r_{i(j-1)} + r_{i(j+1)} = 1 \\ \end{aligned}\] <p>Hub在文中所起的作用类似于Steiner点，但是Hub的生成和连接方法与RSMT有所不同。</p> <h2 id="methodology">Methodology</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper_reading/HUB-480.webp 480w,/assets/img/paper_reading/HUB-800.webp 800w,/assets/img/paper_reading/HUB-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper_reading/HUB.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="hub-generation">Hub Generation</h3> <p>Hub的生成使用的是条件生成模型，输出以生成Hub为主，同时还生成了用于过滤噪声点的stripe mask和未被使用的预路由路径。</p> <p>stripe是一个布尔值矩阵，可用于拒绝部分生成的错误Hub。在具体过滤时，stripe mask按行或列进行计算，如果行或列上有超过半数的mask为真，则该行或列被保留。所以stripe mask是一个部分列或行全部为真的矩阵。</p> <h3 id="pin-hub-connection">Pin-hub Connection</h3> <p>连接hub和pin被视为一个RSMT构建问题，引入hub的构建方法的好处是</p> <ol> <li>可以降低复杂度至\(O(n\log n)\)</li> <li>RSMT只能找到最短路径，引入hub可以支持其他约束条件</li> </ol> <p>具体地，参考REST<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>学习Rectilinear edge sequence (RES)的方法，应用了actor critic算法。actor根据给定的点坐标生成RES，critic预测RSMT的长度，这个预测值在训练时向真实值靠拢，以达到优化目的。</p> <h2 id="experiment">Experiment</h2> <h3 id="dataset">Dataset</h3> <p>在routing benchmark ISPD-07上应用NCTU-GR<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>获取真实的路由示例。额外引入的路由数据集包括ISPD-98、DRL-8和DRL-16.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>J. Liu, G. Chen, and E. F. Young. Rest: Constructing rectilinear steiner minimum tree via reinforcement learning. In 2021 58th ACM/IEEE Design Automation Conference (DAC), pages 1135–1140. IEEE, 2021. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>W.-H. Liu, W.-C. Kao, Y.-L. Li, and K.-Y. Chao. Nctu-gr 2.0: Multithreaded collision-aware global routing with bounded-length maze routing. IEEE Transactions on computer-aided design of integrated circuits and systems, 32(5):709–722, 2013. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="paper-reading"/><category term="chip"/><category term="RL"/><summary type="html"><![CDATA[Introduciton]]></summary></entry><entry><title type="html">The Policy-gradient Placement and Generative Routing Neural Networks for Chip Design</title><link href="https://truman0102.github.io/blog/2024/generative-routing-chip/" rel="alternate" type="text/html" title="The Policy-gradient Placement and Generative Routing Neural Networks for Chip Design"/><published>2024-12-04T17:00:00+00:00</published><updated>2024-12-04T17:00:00+00:00</updated><id>https://truman0102.github.io/blog/2024/generative-routing-chip</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/generative-routing-chip/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>这篇文章是<a href="/blog/2024/joint-learning-chip">布局学习</a>的续作，在宏放置工作的基础上，提出了混合尺寸宏放置和生成式路由方法。</p> <h2 id="fundamentals">Fundamentals</h2> <h3 id="global-routing-grid-protocol">Global Routing Grid Protocol</h3> <p>在全局路由中，物理芯片被分割成多个矩形网格，每个网格代表一个节点，相邻网格的公共边表示两个节点之间的连接，也将这个节点称作全局路由单元。所有路由都应遵循Rectilinear Steiner Tree (RST)规则，连接路径仅限于水平和垂直线段。</p> <h2 id="methodology">Methodology</h2> <h3 id="placement">Placement</h3> <p>与前一篇工作基本一致，主要有两点改进</p> <ol> <li>考虑了宏的尺寸，在采取放置动作时，选择宏的中心点并在二元矩阵中对整个宏的位置进行标记。</li> <li>在奖励函数中额外引入了对重叠的惩罚。</li> </ol> <p>Placement作为pipeline的第一阶段，与路由任务都接受布线长度作为奖励信号，这里采用了变权的长度估计方法，总结来说就是训练前期采用HPWL，训练后期采用生成模型的路由结果，这样的设计是考虑到前期的布线结果可能不够准确。</p> <h3 id="generative-routing">Generative Routing</h3> <p>输入是基于放置结果的与全局路由单元网格等大的图像，包括三个通道，分别是引脚的位置、水平和垂直网格边缘的可用性，输出是一个图像，其单元格取值表示是否属于路由的概率，对应生成式路由模型。路由生成模型是一个生成对抗网络，它包含一个基模型\(G_{\text{base}}\)和一个处理边长超过\(64\)的大模型\(G_{\text{large}}\)。\(G_{\text{base}}\)的热力图和\(G_{\text{large}}\)的热力图相加用于生成更大的路由图。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper_reading/cGAN-480.webp 480w,/assets/img/paper_reading/cGAN-800.webp 800w,/assets/img/paper_reading/cGAN-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper_reading/cGAN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>判别器评估路由结果的连通性和真实性，这是路由任务的主要对抗损失。此外，作者采用了focal loss去学习大量琐碎的否定点和少量的正点。考虑到HPWL可以作为路由的理论下界，可以将路由结果的长度与半周长的差值作为限制线长的正则项。</p> <h3 id="learning-net-order-to-route">Learning Net Order to Route</h3> <h2 id="experiment">Experiment</h2> <h3 id="benchmark">Benchmark</h3> <p>ISPD-2005 for placement and ISPD-07 for routing</p>]]></content><author><name></name></author><category term="paper-reading"/><category term="chip"/><category term="RL"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">On Joint Learning for Solving Placement and Routing in Chip Design</title><link href="https://truman0102.github.io/blog/2024/joint-learning-chip/" rel="alternate" type="text/html" title="On Joint Learning for Solving Placement and Routing in Chip Design"/><published>2024-12-04T15:00:00+00:00</published><updated>2024-12-04T15:00:00+00:00</updated><id>https://truman0102.github.io/blog/2024/joint-learning-chip</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/joint-learning-chip/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>布局布线任务主要由三部分组成，分别是宏单元的放置、标准单元的放置和路由，这篇文章的主要工作是提出了应用强化学习方法的宏单元放置方法，至于标准单元的放置和路由，则采用了现有的DREAMPlace和DeepPR方法。下面将围绕强化学习方法介绍主要内容。</p> <h2 id="methodology">Methodology</h2> <h3 id="state">State</h3> <p>状态分为两部分，分别是描述全局的布局图\(I\)和包含已放置宏的详细位置的网表图\(H\)，\(I\)是一个二进制矩阵，用于表示单元格是否被占用。</p> <h3 id="action">Action</h3> <p>RL agent每次可以选择一个宏单元进行放置，放置的位置由一个二维坐标\((x, y)\)表示。因此动作空间是\(I\)中所有未被占用的位置。</p> <h3 id="reward">Reward</h3> <p>奖励分为外部奖励和内部奖励，外部奖励粗略估计布局布线的质量，内部奖励用于解决稀疏奖励问题，鼓励agent进行探索。</p> <p>外部奖励由线长和拥塞组成，真实线长取决于路由结果，为了快速评估，所以用半周长线长(HPWL)代替。拥塞通过<a href="https://circuitnet.github.io/feature/routability%20features.html#rudy-%E2%91%AA--%E2%91%AE">矩形均匀线密度</a>(RUDY)计算，文中路由拥塞的阈值设为0.1。</p> \[R_E = -\text{WireLength}(P,H) - \lambda\text{Congestion}(P,H)\] <p>其中\(P\)是放置结果，\(H\)是网表图，\(\lambda\)是权重。</p> <p>内部奖励采用了随机网络蒸馏(Random Network Distillation, RND)的方法，用于解决稀疏奖励问题。RND的目标是预测随机网络的输出，奖励是预测误差。</p> \[R_I = \Vert \hat{f}_\theta(o) - f(o) \Vert^2\] <h3 id="model">Model</h3> <p>策略网络的输入是\(I\)和\(H\)，使用了一个CNN和一个GNN对状态分别进行编码，然后将两个编码结果拼接，输入到一个MLP中，输出动作的概率分布，实现了PPO算法。</p> <h3 id="pretraining">Pretraining</h3> <p>类似课程学习的思想，预训练阶段只对宏单元放置进行训练，学习一个中间位置。</p> <h2 id="experiment">Experiment</h2> <p>Benchmark选择的是ISPD-2005</p>]]></content><author><name></name></author><category term="paper-reading"/><category term="chip"/><category term="RL"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Macro Placement for Chip Design</title><link href="https://truman0102.github.io/blog/2024/macro-placement/" rel="alternate" type="text/html" title="Macro Placement for Chip Design"/><published>2024-12-03T14:30:00+00:00</published><updated>2024-12-03T14:30:00+00:00</updated><id>https://truman0102.github.io/blog/2024/macro-placement</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/macro-placement/"><![CDATA[<h2 id="what-are-macros">What are macros?</h2> <p>宏是较大的功能块，例如 IC 的存储器（SRAM、ROM）、模拟设备（DAC、ADC）、时钟宏（PLL）或接口宏。在物理设计中，宏通常被放置在芯片的外围，以确保它们能够获得足够的电源和散热。</p> <h2 id="guidelines-to-place-macros">Guidelines to place macros</h2> <ol> <li>根据飞线分析来放置宏，即与某些端口或频繁通信的宏，然后将它们放置在彼此附近以减少延迟。优先顺序为宏到端口&gt;宏到宏&gt;宏到标准单元。</li> <li>宏的放置遵循分层命名约定。</li> <li>将宏放置在芯片外围，这样更容易为它们供电，避免电压下降。如果将它们放置在芯片中央，可能会导致大量绕行布线。</li> <li>保持宏之间的最小通道长度</li> <li>宏之间不应该有重叠的区域或交叉连接</li> <li>宏引脚应朝向核心区域放置。原因是宏与标准单元相连，而大多数逻辑连接都在核心区域内，因此宏引脚应朝向核心区域放置。</li> <li>避免在宏放置中出现缺口。</li> </ol> <h2 id="why-we-place-macro-before-standard-cell-in-physical-design">Why we place macro before standard cell in physical design?</h2> <p>在集成电路的物理设计流程中，将宏单元在标准单元之前放置是一个经过深思熟虑的策略，背后有着多个具体原因和考虑：</p> <ol> <li> <p><strong>占用面积大</strong>：宏单元通常包括大规模的存储器模块或专用的IP模块，这些模块在芯片上需要占据很大的面积。由于其体积庞大，一旦确定位置后，很难再进行调整。因此，在设计初期就需要为它们预留足够的空间，以避免后期设计调整带来的复杂性。</p> </li> <li> <p><strong>固定布局的约束</strong>：由于宏单元的尺寸较大，而且可能有严格的性能、热管理和电力传输要求，它们通常会受到许多布局约束。这些约束要求在设计的早期阶段予以解决，这样可以降低对后续步骤的影响。</p> </li> <li> <p><strong>供电和散热考虑</strong>：宏单元由于设计的复杂性，通常会有更高的功耗和更复杂的供电需求。先放置这些单元并进行供电布线有利于优化电源分配，同时也可以进行早期的热分析，以确保芯片的可靠性和性能。</p> </li> <li> <p><strong>信号完整性和时序优化</strong>：宏单元可能会引入较长的信号路径，它们的相对位置将会直接影响到信号的延迟和完整性。在早期进行放置，能够更好地规划总线和关键信号路径，避免后期的重布局或时序问题。</p> </li> <li> <p><strong>设计的灵活性</strong>：通过首先确定宏单元的位置，可以在更紧凑和高效的空间中安排标准单元。这种方法允许对标准单元的布局进行更灵活的优化，同时避免对宏单元的重新排列造成的复杂调整。</p> </li> <li> <p><strong>提高设计效率</strong>：宏单元的早期放置和固定，使后续的设计流程更加流畅。当宏单元位置确定后，物理设计中的后续步骤（例如标准单元的放置、详细布线及优化等）可以在一个较为稳定的框架下进行，减少了因宏单元位置变化带来的不确定性。</p> </li> </ol> <p>通过在物理设计流程的初期阶段优先考虑宏单元的位置和布局，不仅可以确保设计满足技术和性能的要求，还能大幅提高整个设计流程的效率和效果。</p>]]></content><author><name></name></author><category term="chip-design"/><category term="chip"/><category term="EDA"/><summary type="html"><![CDATA[brief introduction to macro placement in chip design]]></summary></entry><entry><title type="html">Diffusion Model</title><link href="https://truman0102.github.io/blog/2024/diffusion-model/" rel="alternate" type="text/html" title="Diffusion Model"/><published>2024-09-14T04:00:00+00:00</published><updated>2024-09-14T04:00:00+00:00</updated><id>https://truman0102.github.io/blog/2024/diffusion-model</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/diffusion-model/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Diffusion model is defined as a Markov chain of diffusion steps to slowly add random noise to data and learn to reverse the process to construct data from the noise.</p> <h2 id="definition">Definition</h2> <h3 id="forward-process">Forward Process</h3> <p>Given a data \(x_0\sim q(x)\), a sequence of noisy samples \(x_1, x_2, \ldots, x_T\) is generated by adding gaussian noise to the data step by step. The step sizes are controlled by a variance schedule \(\{\beta_t\in (0,1)\}_{t=1}^T\).</p> \[\begin{aligned} q(x_t\vert x_{t-1}) &amp;= \mathcal{N}(x_t\vert \sqrt{1-\beta_t}x_{t-1}, \beta_t I) \\ q(x_{1\colon T}\vert x_0) &amp;= \prod_{t=1}^T q(x_t\vert x_{t-1}) \end{aligned}\] <p>As \(T\to\infty\), the distribution of \(x_T\) approaches an isotropic gaussian distribution. Let \(\alpha_t = 1-\beta_t\) and \(\bar{\alpha}_t = \prod_{s=1}^t\alpha_s\), the above process can be written as a closed-form solution using repameterization trick:</p> \[\begin{aligned} q(x_t\vert x_{t-1}) &amp;= \mathcal{N}(x_t\vert \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I)&amp; \\ &amp;= \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1}&amp; \epsilon_{t-1}\sim\mathcal{N}(0, I) \\ &amp;= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon_{t-2} + \sqrt{1-\alpha_t}\epsilon_{t-1}&amp; \\ &amp;= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\bar{\epsilon}_{t-2}&amp; \bar{\epsilon}_{t-2}\text{ is a mixture of }\epsilon_{t-1}\text{ and }\epsilon_{t-2} \\ &amp;= \dots &amp; \\ &amp;= \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon&amp; \\ q(x_t\vert x_0) &amp;= \mathcal{N}(x_t\vert \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I) \end{aligned}\] <p>Two gaussians \(\mathcal{N}(0, \sigma_1^2 I)\) and \(\mathcal{N}(0, \sigma_2^2 I)\) can be mixed into a gaussian \(\mathcal{N}(0, (\sigma_1^2+\sigma_2^2)I)\). The variance of the gaussian \(x_t\) refers to the sum of the variances of the noise at each step, which is \(\sum_{s=1}^t\beta_s\).</p> <h3 id="reverse-process">Reverse Process</h3> <p>A model \(p_{\theta}\) is trained to reverse the process by learning the conditional distribution \(p_{\theta}(x_{t-1}\vert x_t)\).</p> \[p_{\theta}(x_{t-1}\vert x_t) = \mathcal{N}(x_{t-1}\vert \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\] <p>The conditional probability can be written as conditioned on \(x_0\):</p> \[\begin{aligned} q(x_{t-1}\vert x_t, x_0) &amp;= \mathcal{N}(x_{t-1}\vert \tilde{\mu}_(x_t, x_0), \tilde{\beta}_t I) \\ \end{aligned}\] <p>According bayes’ rule, the mean and variance can be parameterized as follows:</p> \[\begin{aligned} \tilde{\beta}_t &amp;= \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t \\ \tilde{\mu}_t(x_t, x_0) &amp;= \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 \\ x_0 &amp;= \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon) \\ \tilde{\mu} &amp;= \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon) \\ &amp;= \frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon) \end{aligned}\] <p>Thus, the reverse process can be written as:</p> \[x_{t-1} = \mathcal{N}(x_{t-1}\vert \frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t, t)), \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t I)\] <p>It can be shown that all we need to do is to learn the noise \(\epsilon_\theta(x_t, t)\), equivalent to learning the noise \(\epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)\)</p> <h2 id="advanced-topics">Advanced Topics</h2> <h3 id="parameterization-of-beta">Parameterization of \(\beta\)</h3> <p>Usually, we can afford a larger update step when the sample gets noisier, so \(\beta_1&lt; \beta_2&lt; \ldots &lt; \beta_T\) and therefore \(\bar{\alpha}_1 &gt; \bar{\alpha}_2 &gt; \ldots &gt; \bar{\alpha}_T\).</p> <h3 id="parameterization-of-variance">Parameterization of Variance</h3> <p>Since learning a variance leads to unstable training, the variance is usually set to a fixed value \(\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\).</p> <h3 id="denoising-diffusion-implicit-model">Denoising Diffusion Implicit Model</h3> <p>DDIM makes it possible to train the diffusion model up to any arbitrary number of forward steps but only sample from a subset of steps in the generative process.</p> \[q_{\sigma, s&lt;t}(x_s\vert x_t, x_0) = \mathcal{N}(x_s\vert \sqrt{\bar{\alpha}_s}(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta^t(x_t)}{\sqrt{\bar{\alpha}_t}})+\sqrt{1-\bar{\alpha}_s-\sigma_t^2}\epsilon_\theta^t(x_t), \sigma_t^2I)\] <h3 id="latent-variable-space">Latent Variable Space</h3> <p>Latent diffusion model runs the process in a latent space \(z_t\) instead of the data space \(x_t\), which is more efficient and can be used to process multi-modal data.</p> <h3 id="model-architecture">Model Architecture</h3> <p>There are two common architectures for diffusion models: U-Net and Transformer.</p> <p>For more details, please refer to <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian Weng’s blog</a>.</p>]]></content><author><name></name></author><category term="data-science"/><category term="math"/><summary type="html"><![CDATA[An introduction to the diffusion model.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://truman0102.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://truman0102.github.io/blog/2024/tabs</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="f5c56ad8-8017-4c85-a19b-ce7a632bb8d6" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="f5c56ad8-8017-4c85-a19b-ce7a632bb8d6" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="67a0b3a3-1abe-4e3a-bd2f-8bc31551076e" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="67a0b3a3-1abe-4e3a-bd2f-8bc31551076e" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="59c6fa5e-6013-4040-8e0c-9281b953d54b" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="59c6fa5e-6013-4040-8e0c-9281b953d54b" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://truman0102.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://truman0102.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://truman0102.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="rouge-code"><pre><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry></feed>